https://github.com/wangyuqing4/alluxio-k8s/tree/master/singleMaster-localJournal
-------------------------------------------------------------
import socket
import os
from pyspark import SparkConf
from pyspark import SparkContext
from pyspark.sql import SparkSession

master = "spark://spark:7077"
driver_host = socket.gethostbyname(socket.gethostname())
print (f"master:{master}\ndriver_host:{driver_host}")

sc = SparkContext (conf=SparkConf().setAll ([
    ("spark.master",               master),
    ("spark.driver.host",          driver_host),
    ("spark.app.name",             "jib"),
    ("spark.submit.deployMode",    "client"),
    ("spark.driver.memory",        "1g"),    
    ("spark.executor.memory",      "512M")
]))
r = sc.parallelize ([0, 8, 4]).map(lambda v: v * 6).collect ()
spark = SparkSession (sc)
print (r)
-------------------------------------------------------------

import requests

url = "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-20/noncomm_use_subset.tar.gz"
covid_19__non_comm_use_tgz = 'noncomm_use_subset.tar.gz'
response = requests.get(url, stream=True)
if response.status_code == 200:
    with open(covid_19__non_comm_use_tgz, 'wb') as f:
        f.write(response.raw.read())
-------------------------------------------------------------

import traceback
from minio import Minio
from minio.error import ResponseError

class Storage:
    """ Storage abstraction. """
    def __init__(self, 
                 endpoint="minio:9000",
                 access_key="minio", 
                 secret_key="minio123", 
                 secure=False
    ):    
        """ Port is required. """
        self.minio_endpoint = "minio:9000"
        self.minio_access_key = "minio"
        self.minio_secret_key = "minio123"
        print (f"minio_endpoint:{self.minio_endpoint}")
    
        self.client = Minio(
            self.minio_endpoint,
            access_key=self.minio_access_key,
            secret_key=self.minio_secret_key,
            secure=False)        
    def exists (self, bucket):
        return self.client.bucket_exists (bucket)
    def make_bucket (self, bucket):
        return self.client.make_bucket(bucket)
    def list_buckets (self):
        return client.list_buckets()
    def fput_object (self, bucket, source, target):
        return self.client.fput_object(bucket, source, target)
    def list_objects (self, bucket, prefix=None, recursive=False):
        return self.client.list_objects_v2(bucket, prefix, recursive)
    def remove_object (self, bucket, object_name):
        return self.client.remove_object (bucket, object_name)
-------------------------------------------------------------    
bucket = 'covid-19'
data = 'test.txt' #'noncomm_use_subset.tar.gz'
store = Storage ()
if not store.exists (bucket):
    print (f"creating bucket {bucket}")
    store.make_bucket (bucket)
data_set = [ i.object_name for i in store.list_objects (bucket, recursive=True) ]
if data in data_set:
    print (f"object {data} is in bucket {bucket}")
else:
    print (f"creating data object {data}")
    store.fput_object (bucket, data, data)
-------------------------------------------------------------
import tarfile
import json
import minio

objects = []
with tarfile.open("noncomm_use_subset.tar.gz", "r:gz") as tar:
    for index, member in enumerate(tar.getmembers()):
        if index > 5:
            break
        stream = tar.extractfile(member)
        objects.append (stream.read ()) #json.load(stream))
#x = json.loads(objects[0])
#print (json.dumps(x, indent=2))


r = sc.parallelize ([0, 8, 4]).map(lambda v: v * 6).collect ()
print (r)
'''
objs = [{ 
    "id" : x['paper_id'],
    "text" : [ obj['text'] for obj in x['body_text'] ]
} for x in [ json.loads(o) for o in objects ] ]
print ("sparking")
r = sc.parallelize (objs)
print ("framing")
spark.read.json (r)
'''
-------------------------------------------------------------
# spark-s3
# https://www.jitsejan.com/setting-up-spark-with-minio-as-object-storage.html
sc = SparkContext (conf=SparkConf().setAll ([
    ("spark.master",                   master),
    ("spark.driver.host",              driver_host),
    ("spark.app.name",                 "jib-s3"),
    ("spark.jars.packages",            "org.apache.hadoop:hadoop-aws:2.7.3,com.amazonaws:aws-java-sdk:1.10.8"),
    ("spark.hadoop.fs.s3a.endpoint",   f"http://{store.endpoint}"),
    ("spark.hadoop.fs.s3a.access.key", store.access_key),
    ("spark.hadoop.fs.s3a.secret.key", store.secret_key),
    ("spark.hadoop.fs.s3a.path.style.access", "true"),
    ("spark.hadoop.fs.s3a.impl",       "org.apache.hadoop.fs.s3a.S3AFileSystem"),
    ("spark.submit.deployMode",        "client"),
    ("spark.driver.memory",            "1g"),    
    ("spark.executor.memory",          "512M")
]))
dataset = sc.textFile("s3a://covid-19/test.txt")
dataset.map(lambda v: v * 6).collect ()

print (r)
----------------------------------------------------------------
# https://www.jitsejan.com/using-spark-to-read-from-s3.html
import socket
import os
from pyspark import SparkConf
from pyspark import SparkContext
from pyspark.sql import SparkSession

sc.stop ()
driver_host = socket.gethostbyname(socket.gethostname())
K8S_API_HOST="kubernetes.default.svc" #"scox.europa.renci.org"
NAMESPACE = "jib" #open("/var/run/secrets/kubernetes.io/serviceaccount/namespace").read()
sc = SparkContext (conf=SparkConf().setAll ([
    ("spark.master",                   f"k8s://http://{K8S_API_HOST}:8080" ),
    ("spark.driver.host",              driver_host),
    ("spark.app.name",                 "jib-s3"),
    ("spark.jars.packages",            "org.apache.hadoop:hadoop-aws:2.7.3,com.amazonaws:aws-java-sdk:1.7.4"),
    ("spark.hadoop.fs.s3a.endpoint",   f"http://{store.endpoint}"),
    ("spark.hadoop.fs.s3a.access.key", store.access_key),
    ("spark.hadoop.fs.s3a.secret.key", store.secret_key),
    ("spark.hadoop.fs.s3a.path.style.access", "true"),
    ("spark.hadoop.fs.s3a.impl",       "org.apache.hadoop.fs.s3a.S3AFileSystem"),
    ("spark.submit.deployMode",        "client"),
    ("spark.executor.instances",       "2"),
    ("spark.driver.memory",            "500M"),
    ("spark.executor.memory",          "512M"),
    ("spark.kubernetes.namespace",     NAMESPACE),
    ("spark.kubernetes.authenticate.driver.serviceAccountName", "spark"),
    ("spark.kubernetes.container.image",                        "spark-alluxio"),
    #    ("spark.kubernetes.executor.volumes.hostPath.alluxio-domain.mount.path",     "/opt/domain"),
    #    ("spark.kubernetes.executor.volumes.hostPath.alluxio-domain.mount.readOnly", "true"),
    #    ("spark.kubernetes.executor.volumes.hostPath.alluxio-domain.options.path",   "/tmp/alluxio-domain"),
    #    ("spark.kubernetes.executor.volumes.hostPath.alluxio-domain.options.type",   "Directory")
]))
dataset = sc.textFile("s3a://covid-19/test.txt")
dataset.map(lambda v: v * 6).collect ()

print (r)

-----------------------------------------------------------------------
import socket
import os
from pyspark import SparkConf
from pyspark import SparkContext
from pyspark.sql import SparkSession

sc.stop ()
driver_host = socket.gethostbyname(socket.gethostname())
K8S_API_HOST="kubernetes.default.svc" #"scox.europa.renci.org"
NAMESPACE = "jib" #open("/var/run/secrets/kubernetes.io/serviceaccount/namespace").read()
sc = SparkContext (conf=SparkConf().setAll ([
    ("spark.master",                   f"k8s://http://{K8S_API_HOST}:8080" ),
    ("spark.driver.host",              driver_host),
    ("spark.app.name",                 "Jib S3"),
    ("spark.submit.deployMode",        "client"),
    ("spark.executor.instances",       "2"),
    ("spark.driver.memory",            "500M"),
    ("spark.executor.memory",          "512M"),
    ("spark.kubernetes.namespace",     NAMESPACE),
    ("spark.kubernetes.authenticate.driver.serviceAccountName", "spark"),
    ("spark.kubernetes.container.image",                        "spark-alluxio")
]))
print ("2. -----------------")
dataset = sc.textFile("alluxio://alluxio-master-0:19998/LICENSE")
dataset.map(lambda v: v * 6).collect ()

print (r)


--------------------------------------------------------
n1-blackbalsam.blackbalsam-cluster.edc.renci.org is the first of 5, and there's a nfs host with same domain.
2:01
"/etc/kubernetes/admin.conf" on n1 is the kubeconfig.



-----------------------------
$ kc exec -it alluxio-master-0 -- /bin/bash
bash-4.4$ alluxio fsadmin report capacity
Capacity information for all workers: 
    Total Capacity: 5120.00MB
        Tier: MEM  Size: 5120.00MB
    Used Capacity: 0B
        Tier: MEM  Size: 0B
    Used Percentage: 0%
    Free Percentage: 100%

Worker Name                                                   Last Heartbeat   Storage       MEM
n3-blackbalsam-cluster.blackbalsam-cluster.edc.renci.org      0                capacity      1024.00MB
                                                                               used          0B (0%)
n4-blackbalsam-cluster.blackbalsam-cluster.edc.renci.org      0                capacity      1024.00MB
                                                                               used          0B (0%)
n1-blackbalsam-cluster.blackbalsam-cluster.edc.renci.org      0                capacity      1024.00MB
                                                                               used          0B (0%)
n5-blackbalsam-cluster.blackbalsam-cluster.edc.renci.org      0                capacity      1024.00MB
                                                                               used          0B (0%)
n2-blackbalsam-cluster.blackbalsam-cluster.edc.renci.org      0                capacity      1024.00MB
                                                                               used          0B (0%)
bash-4.4$ alluxio fs copyFromLocal LICENSE /
Apr 05, 2020 3:22:17 PM io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
WARNING: [Channel<9>: (n2-blackbalsam-cluster.blackbalsam-cluster.edc.renci.org:29999)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host n2-blackbalsam-cluster.blackbalsam-cluster.edc.renci.org, cause=java.lang.RuntimeException: java.net.UnknownHostException: n2-blackbalsam-cluster.blackbalsam-cluster.edc.renci.org
	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:441)
	at io.grpc.internal.DnsNameResolver$Resolve.resolveInternal(DnsNameResolver.java:272)
	at io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:228)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.UnknownHostException: n2-blackbalsam-cluster.blackbalsam-cluster.edc.renci.org
	at java.net.InetAddress.getAllByName0(InetAddress.java:1281)
	at java.net.InetAddress.getAllByName(InetAddress.java:1193)
	at java.net.InetAddress.getAllByName(InetAddress.java:1127)
	at io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:651)
	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:409)
	... 5 more
}
Failed to cache: Target Unavailable. GrpcChannelKey{ClientType=DefaultBlockWorkerClient, ClientHostname=alluxio-master-0.alluxio-master.blackbalsam.svc.blackbalsam-cluster.edc.renci.org, ServerAddress=GrpcServerAddress{HostName=n2-blackbalsam-cluster.blackbalsam-cluster.edc.renci.org, SocketAddress=n2-blackbalsam-cluster.blackbalsam-cluster.edc.renci.org:29999}, ChannelId=97e1273c-ab87-4eb8-b2ff-2717d8373f0c}


bash-4.4$ wget n1-blackbalsam-cluster.edc.renci.org
Connecting to n1-blackbalsam-cluster.edc.renci.org (172.25.16.201:80)
wget: can't connect to remote host (172.25.16.201): Connection refused
bash-4.4$ wget stars-c0.edc.renci.org
Connecting to stars-c0.edc.renci.org (172.25.8.116:80)
wget: can't connect to remote host (172.25.8.116): Connection refused
bash-4.4$ wget n1-blackbalsam.blackbalsam-cluster.edc.renci.org
wget: bad address 'n1-blackbalsam.blackbalsam-cluster.edc.renci.org'


