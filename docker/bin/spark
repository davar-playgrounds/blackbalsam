#!/bin/bash

set -x
set -e

# Configure environment
export ALLUXIO_VERSION_NUM=2.2.0
export ALLUXIO_VERSION=alluxio-2.2.0    
export SPARK_PY_VERSION=0.0.1
export SPARK_VERSION=2.4.5
export HADOOP_VERSION=3.1.3
export SPARK_HOME=$PWD/spark-${SPARK_VERSION}-bin-without-hadoop
export HADOOP_HOME=$PWD/hadoop-${HADOOP_VERSION}
export APP_HOME=${APP_HOME:-$PWD}

DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
APP_HOME=$( dirname $DIR )
echo $APP_HOME

#export SPARK_DIST_CLASSPATH_ARG='/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*'

######################################################################
##
##  Build a Spark-Alluxio docker image.
##    * Add configuration to incorporate Hadoop libraries.
##    * Get the appropriate AWS libraries for S3 integration.
##    * Add all of this to our Spark distribution. 
##    * Build a docker image packaging it all.
##    * Login to docker and push the image.
##
######################################################################
configure_alluxio () {
    # Configure Alluxio
    if [ ! -f $SPARK_HOME/jars/$ALLUXIO_VERSION-client.jar ]; then
        id=$(docker create alluxio/alluxio:$ALLUXIO_VERSION_NUM)
        docker cp $id:/opt/alluxio/client/$ALLUXIO_VERSION-client.jar $SPARK_HOME/jars/$ALLUXIO_VERSION-client.jar
        docker rm -v $id 1>/dev/null
    fi
    cp $APP_HOME/blackbalsam-jupyter/core-site.xml $SPARK_HOME/conf
}
build-spark () {
    # Get spark, hadoop, unpack, and clean up.
    $APP_HOME/blackbalsam-jupyter/libspark get_spark clean

    # Get Alluxio jar and config files.
    configure_alluxio
}
build-images () {
    # Build Docker images
    source ~/.dockerhub
    docker login -u $DOCKERHUB_USERNAME -p $DOCKERHUB_PASSWORD    
    cd $SPARK_HOME
    pwd
    for op in build push; do
	./bin/docker-image-tool.sh \
            -r renciorg \
            -t $SPARK_PY_VERSION \
            -b spark \
            -p $APP_HOME/spark/Dockerfile.pyspark $op
#            -b SPARK_DIST_CLASSPATH_ARG \
    done
}
build () {
    build-spark
    build-images
}

$*
