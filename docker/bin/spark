#!/bin/bash

set -x
set -e

f () {
export NAMESPACE=blackbalsam
export K8S_API_HOST=0.0.0.0
export K8S_API_HOST=localhost
export K8S_API_HOST=127.0.0.1
export ALLUXIO_MASTER=alluxio-master-0
export K8S_API_PORT=6443
export K8S_API_HOST=n1-blackbalsam.blackbalsam-cluster.edc.renci.org
export K8S_API_PROTO=https
export K8S_API_ENDPOINT=$K8S_API_PROTO://$K8S_API_HOST:$K8S_API_PORT
export K8S_API_ENDPOINT=https://172.25.16.202:6443

export SPARK_VERSION_NUM=2.4.5
export SPARK_VERSION=spark-$SPARK_VERSION_NUM
export APP_HOME=$PWD
export SPARK_DIST=$SPARK_VERSION-bin-hadoop2.7
export SPARK_HOME=$PWD/$SPARK_DIST
export ALLUXIO_VERSION_NUM=2.2.0
export ALLUXIO_VERSION=alluxio-2.2.0
export SPARK_PY_VERSION=0.0.1
}
# Configure environment
export ALLUXIO_VERSION_NUM=2.2.0
export ALLUXIO_VERSION=alluxio-2.2.0    
export SPARK_PY_VERSION=0.0.1
export SPARK_VERSION=2.4.5
export HADOOP_VERSION=3.1.3
export SPARK_HOME=$PWD/spark-${SPARK_VERSION}-bin-without-hadoop
export HADOOP_HOME=$PWD/hadoop-${HADOOP_VERSION}
export APP_HOME=$PWD

######################################################################
##
##  Build a Spark-Alluxio docker image.
##    * Add configuration to incorporate Hadoop libraries.
##    * Get the appropriate AWS libraries for S3 integration.
##    * Add all of this to our Spark distribution. 
##    * Build a docker image packaging it all.
##    * Login to docker and push the image.
##
######################################################################
configure_alluxio () {
    # Configure Alluxio
    if [ ! -f jars/$ALLUXIO_VERSION-client.jar ]; then
        id=$(docker create alluxio/alluxio:$ALLUXIO_VERSION_NUM)
        docker cp $id:/opt/alluxio/client/$ALLUXIO_VERSION-client.jar $SPARK_HOME/jars/$ALLUXIO_VERSION-client.jar
        docker rm -v $id 1>/dev/null
    fi
    cp blackbalsam-jupyter/core-site.xml $SPARK_HOME/conf
}
build () {
    # Get spark, hadoop, unpack, and clean up.
    blackbalsam-jupyter/libspark get_spark clean

    # Get Alluxio jar and config files.
    configure_alluxio

    # Build Docker images
    source ~/.dockerhub
    docker login -u $DOCKERHUB_USERNAME -p $DOCKERHUB_PASSWORD    
    cd $SPARK_HOME
    pwd
    for op in build push; do
	./bin/docker-image-tool.sh -r renciorg -t $SPARK_PY_VERSION -b spark -p $APP_HOME/spark/Dockerfile.pyspark $op
    done
}

$*
