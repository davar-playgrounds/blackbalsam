#!/bin/bash

set -x
set -e

export NAMESPACE=blackbalsam

export K8S_API_HOST=0.0.0.0
export K8S_API_HOST=localhost
export K8S_API_HOST=127.0.0.1
export ALLUXIO_MASTER=alluxio-master-0

export K8S_API_PORT=6443
export K8S_API_HOST=n1-blackbalsam.blackbalsam-cluster.edc.renci.org
export K8S_API_PROTO=https
export K8S_API_ENDPOINT=$K8S_API_PROTO://$K8S_API_HOST:$K8S_API_PORT
export K8S_API_ENDPOINT=https://172.25.16.202:6443

export SPARK_VERSION_NUM=2.4.5
export SPARK_VERSION=spark-$SPARK_VERSION_NUM
export APP_HOME=$PWD
export SPARK_DIST=$SPARK_VERSION-bin-hadoop2.7
export SPARK_HOME=$PWD/$SPARK_DIST
export ALLUXIO_VERSION_NUM=2.2.0
export ALLUXIO_VERSION=alluxio-2.2.0
export SPARK_PY_VERSION=0.0.4

init () {
    if [ ! -d $SPARK_HOME ]; then
	cd $APP_HOME
        wget --timestamping --quiet http://apache.mirrors.pair.com/spark/$SPARK_VERSION/$SPARK_DIST.tgz
        tar -xf $SPARK_DIST.tgz
    fi
    cd $SPARK_DIST
}

get_alluxio () {
    if [ ! -f jars/$ALLUXIO_VERSION-client.jar ]; then
        id=$(docker create alluxio/alluxio:$ALLUXIO_VERSION_NUM)
        docker cp $id:/opt/alluxio/client/$ALLUXIO_VERSION-client.jar jars/$ALLUXIO_VERSION-client.jar
        docker rm -v $id 1>/dev/null
    fi
}
######################################################################
##
##  Build a Spark-Alluxio docker image.
##    * Add configuration to incorporate Hadoop libraries.
##    * Get the appropriate AWS libraries for S3 integration.
##    * Add all of this to our Spark distribution. 
##    * Build a docker image packaging it all.
##    * Login to docker and push the image.
##
######################################################################
REPO=https://repo1.maven.org/maven2
get_dep () {
    echo getting $1...
    wget --timestamping --quiet $1 -P $2
}
get_spark_dep () {
    get_dep $1 $SPARK_HOME/jars
}
get_spark_dep_group () {
    deps="$1"
    dep_group=$2
    dep_group_ver=$3
    for dep in $deps; do
	get_spark_dep $REPO/${dep_group}/$dep/$dep_group_ver/$dep-$dep_group_ver.jar
    done
}
get_spark_deps () {
    deps="aws-java-sdk-kms aws-java-sdk-s3 aws-java-sdk-dynamodb aws-java-sdk-core aws-java-sdk"
    dep_group="com/amazonaws"
    dep_group_ver=1.11.534
    get_spark_dep_group "$deps" $dep_group $dep_group_ver
    
    deps="hadoop-aws"
    dep_group="org/apache/hadoop"
    dep_group_ver=3.1.2
    get_spark_dep_group "$deps" $dep_group $dep_group_ver
}
build_spark () {
    cp ../core-site.xml $SPARK_HOME/conf
    get_spark_deps
    x () {
    MAVEN_REPO=https://repo1.maven.org/maven2
    wget --quiet \
	 --timestamping $MAVEN_REPO/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar \
	-P $SPARK_HOME/jars/
    wget --quiet \
	 --timestamping $MAVEN_REPO/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar \
	-P $SPARK_HOME/jars/
    }
    source ~/.dockerhub
    docker login -u $DOCKERHUB_USERNAME -p $DOCKERHUB_PASSWORD    
    ./bin/docker-image-tool.sh -r renciorg -t $SPARK_PY_VERSION -b spark -p $APP_HOME/Dockerfile.pyspark build
    ./bin/docker-image-tool.sh -r renciorg -t $SPARK_PY_VERSION -b spark -p $APP_HOME/Dockerfile.pyspark push
}
make_account () {
    kubectl delete --ignore-not-found=true --namespace=$NAMESPACE serviceaccount spark
    kubectl delete --ignore-not-found=true --namespace=$NAMESPACE clusterrolebinding spark-role
    kubectl create --namespace=$NAMESPACE serviceaccount spark
    kubectl create --namespace=$NAMESPACE \
            clusterrolebinding spark-role --clusterrole=edit \
            --serviceaccount=$NAMESPACE:spark
}
test () {
    ./bin/spark-submit \
        --master k8s://$K8S_API_ENDPOINT \
        --deploy-mode cluster --name spark-alluxio \--conf spark.executor.instances=1 \
        --class org.apache.spark.examples.JavaWordCount \
        --driver-memory 500m --executor-memory 1g \
        --conf spark.kubernetes.namespace=$NAMESPACE \
        --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
        --conf spark.kubernetes.container.image=renciorg/spark-alluxio:v1.0 \
        --conf spark.kubernetes.executor.volumes.hostPath.alluxio-domain.mount.path=/opt/domain \
        --conf spark.kubernetes.executor.volumes.hostPath.alluxio-domain.mount.readOnly=true \
        --conf spark.kubernetes.executor.volumes.hostPath.alluxio-domain.options.path=/tmp/alluxio-domain \
        --conf spark.kubernetes.executor.volumes.hostPath.alluxio-domain.options.type=Directory \
        local:///opt/spark/examples/jars/spark-examples_2.11-$SPARK_VERSION_NUM.jar \
        alluxio://$ALLUXIO_MASTER:19998/LICENSE
}
test () {
    K8S_API_ENDPOINT="http://n1-blackbalsam.blackbalsam-cluster.edc.renci.org:8080"
    ./bin/spark-submit \
	--master "k8s://$K8S_API_ENDPOINT" \
	--deploy-mode cluster \
	--name spark-test --conf spark.executor.instances=1 \
        --class org.apache.spark.examples.JavaWordCount \
        --driver-memory 500m --executor-memory 1g \
        --conf spark.kubernetes.namespace=$NAMESPACE \
        --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
        --conf spark.kubernetes.container.image=renciorg/spark-py:v0.2 \
        local:///opt/spark/examples/jars/spark-examples_2.11-$SPARK_VERSION_NUM.jar \
        /opt/entrypoint.sh
}

build () {
    get_alluxio
    build_spark
    make_account
}

init

$*
